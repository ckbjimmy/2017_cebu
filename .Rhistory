plot(tsne$Y, col=as.factor(idash$V2))
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
```
idash$V2
plot(tsne$Y, col=as.factor(idash$V2))
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
label
colors <- rainbow(length(unique(as.factor(label))))
text(tsne$Y, labels=label, col=colors[label])
model <- keras_model_sequential()
model %>%
layer_dense(name = 'e1', units = 128, activation = 'relu', input_shape = maxlen) %>%
layer_dense(name = 'e2', units = 64, activation = 'relu') %>%
layer_dense(name = 'e3', units = encoding_dim, activation = 'relu') %>%
layer_dense(name = 'd1', units = 64, activation = 'relu') %>%
layer_dense(name = 'd2', units = 128, activation = 'relu') %>%
layer_dense(name = 'd3', units = maxlen, activation = 'sigmoid')
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adadelta"
)
summary(model)
history <- model %>% fit(
x_train, x_train,
batch_size = batch_size,
epochs = epochs,
shuffle = TRUE,
validation_data = list(x_test, x_test)
)
plot(history)
layer_name <- 'e3'
intermediate_layer_model <- keras_model(inputs = model$input,
outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30,
verbose = TRUE, max_iter = 500)
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
model <- keras_model_sequential()
model %>%
layer_dense(name = 'e1', units = encoding_dim, activation = 'relu', input_shape = maxlen) %>%
layer_dense(name = 'd1', units = maxlen, activation = 'sigmoid')
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam"
)
summary(model)
```
Then we use our data to fit the model, `x_train` as training data and `x_test` for validation. You may change `batch_size` and `epochs` to get better results. This model fitting step may take minutes to run.
```{r}
history <- model %>% fit(
x_train, x_train,
batch_size = batch_size,
epochs = epochs,
shuffle = TRUE,
validation_data = list(x_test, x_test)
)
plot(history)
```
Once the autoencoder is constructed and fine-tuned by the data, we can now extract the hidden representation from the model. The hidden representation is the output of encoder, which is `e1` layer. We may use `keras_model` and `predict` functions to get the representation and save it in the variable `intermediate_output`.
```{r}
layer_name <- 'e1'
intermediate_layer_model <- keras_model(inputs = model$input,
outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
dim(intermediate_output)
head(intermediate_output)
```
We can visualize how the documents locate in the vector space using principal component analysis (PCA) (using `princomp` function, `scores[, 1:2]` for first two principal components).
```{r}
colors <- rainbow(length(unique(as.factor(label))))
pca <- princomp(intermediate_output)$scores[, 1:2]
plot(pca, t='n', main="pca")
text(pca, labels=label, col=colors[label])
library(Rtsne)
tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30,
verbose = TRUE, max_iter = 500)
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
setwd("~/git/2017_cebu/")  # need to set their own path or see below
knitr::opts_chunk$set(echo = TRUE)
setwd("~/git/2017_cebu/")  # need to set their own path or see below
.packages <- c("tm", "SnowballC", "topicmodels", "tidytext", "caret", "rpart",
"text2vec", "Rtsne")
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)
if ("keras" %in% rownames(installed.packages()) == FALSE) {
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
}
setwd("~/git/2017_cebu/")  # need to set their own path or see below
.packages <- c("stringr", "tm", "SnowballC", "wordcloud", "ggplot2", "cluster", "fpc")
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)
data <- readChar("/data/id.txt", file.info("id.txt")$size)
data <- readChar("data/id.txt", file.info("id.txt")$size)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/git/2017_cebu/")  # need to set their own path or see below
.packages <- c("stringr", "tm", "SnowballC", "wordcloud", "ggplot2", "cluster", "fpc")
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
fpath_idash <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/idash.txt"
item <- c("LISINOpril 40 MG PO Daily", "lisinopril 40 mg PO Daily", "captopril 6.25 MG PO TID",
"LISINOPRIL", "april", "pril", "labetalol", "Propanolol 1PC STAT",
"Today is April 5th", "Lisinopril captopril")
grep("pril", item, perl=TRUE)
grepl("pril", item, perl=TRUE)
regexpr("pril", item, perl=TRUE)
gregexpr("pril", item, perl=TRUE)
item <- c("LISINOpril 40 MG PO Daily", "lisinopril 40 mg PO Daily", "captopril 6.25 MG PO TID",
"LISINOPRIL", "april", "pril", "labetalol", "Propanolol 1PC STAT",
"Today is April 5th", "Lisinopril captopril")
re <- regexpr("[A-Za-z]*pril", item, perl=TRUE)
regmatches(item, re)
re <- regexpr("[A-Za-z]*pril", item, perl=TRUE)
regmatches(item, re)
re
regmatches(item, re)
sub("[A-Za-z]*pril", "[DELETED]", "Lisinopril captopril")
gsub("[A-Za-z]*pril", "[DELETED]", "Lisinopril captopril")
gsub("([A-Za-z]* )([0-9]+)( [MGmg].*)", "\\2",
c("lisinopril 40 MG PO Daily", "captopril 5 mg PO BID"))
text <- "Indication: Endocarditis. Valvular heart disease."
grepl("Indication: (.*)", text)
gsub("(Indication: )(.*)(.*)", "\\2", text)
gsub("(Indication: )([aA-zZ]+)(.*)", "\\2", text)
gsub("(Indication: [aA-zZ]+. )([aA-zZ ]+)(.*)", "\\2", text)
text <- "
Indication: Endocarditis. Valvular heart disease.
Height: (in) 64
Weight (lb): 170
BSA (m2): 1.83 m2
BP (mm Hg): 92/61
"
grepl("Weight \\(lb\\): (.*?)\n", text)
gsub("(.*Weight \\(lb\\): )([0-9]+)(\n.*)", "\\2", text)
```
fpath
file.info(fpath)$size
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
fpath <- download.file(fpath, method="curl")
.packages <- c("RCurl", "stringr", "tm", "SnowballC", "wordcloud", "ggplot2", "cluster", "fpc")
# install the package if it is not in the local R
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
fpath <- getURL(textConnection(fpath))
fpath <- read.table(textConnection(getURL(fpath)))
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
download.file(url=fpath, destfile='id.txt', method='curl')
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
download.file(url=fpath, destfile='id.txt', method='curl')
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/idash.txt"
download.file(url=fpath, destfile='idash.txt', method='curl')
grepl("Height: \\(in\\) (.*?)\n", text)
gsub("(.*Height: \\(in\\) )([0-9]{2})(\n.*)", "\\2", text)
gsub("(.*BP \\(mm Hg\\): )([0-9]+)(/[0-9]+\n.*)", "\\2", text)
grepl("Height: \\(in\\) (.*?)\n", text)
gsub("(.*Height: \\(in\\) )([0-9]{2})(\n.*)", "\\2", text)
gsub("(.*BP \\(mm Hg\\): )([0-9]+)(/[0-9]+\n.*)", "\\2", text)
has_neuro <- grepl("(Neuro:|NEURO:|Neuro=|Neuro-)", data, perl=TRUE)
neuro <- gsub("(.*Neuro:|.*NEURO:|.*Neuro=|.*Neuro-)(.*?)(\\.$|\n.*)", "\\2", data[has_neuro])
head(neuro)
data <- readChar("id.txt", file.info("id.txt")$size)
# replace "||||END_OF_RECORD [change line] START_OF_RECORD=number||||number||||" to "[split]"
data <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\nSTART_OF_RECORD=[0-9]+\\|\\|\\|\\|[0-9]+\\|\\|\\|\\|\n", " [split] ", data)
# replace the last "||||END_OF_RECORD" to "" since it can't be replaced by the previous line
data <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\n", "", data)
# split the data by identifying the string "[split]"
data <- strsplit(data, " \\[split\\] ")
data <- data[[1]]
has_neuro <- grepl("(Neuro:|NEURO:|Neuro=|Neuro-)", data, perl=TRUE)
neuro <- gsub("(.*Neuro:|.*NEURO:|.*Neuro=|.*Neuro-)(.*?)(\\.$|\n.*)", "\\2", data[has_neuro])
head(neuro)
has_neuro <- grepl("(Neuro: |NEURO: |Neuro= |Neuro- )", data, perl=TRUE)
neuro <- gsub("(.*Neuro: |.*NEURO: |.*Neuro= |.*Neuro- )(.*?)(\\.$|\n.*)", "\\2", data[has_neuro])
# show the first six results
head(neuro)
has_neuro <- grepl("(Neuro:|NEURO:|Neuro=|Neuro-)", data, perl=TRUE)
neuro <- gsub("(.*Neuro: |.*NEURO: |.*Neuro= |.*Neuro- )(.*?)(\\.$|\n.*)", "\\2", data[has_neuro])
# show the first six results
head(neuro)
has_neuro <- grepl("(Neuro:|NEURO:|Neuro=|Neuro-)", data, perl=TRUE)
neuro <- gsub("(.*Neuro:|.*NEURO:|.*Neuro=|.*Neuro-)(.*?)(\\.$|\n.*)", "\\2", data[has_neuro])
# show the first six results
head(neuro)
has_cv <- grepl("(CV:|C/V:|CV=|CV-|cv:)", data, perl=TRUE)
cv <- gsub("(.*CV:|.*C/V:|.*CV=|.*CV-|.*cv:)(.*?)(\\.$|\n.*)", "\\2", data[has_cv])
head(cv)
library(stringr)
str_replace_all(item, ".*pril.*", "[DELETED]")
str_extract_all(item, "[0-9]+")
item
str_replace_all(item, ".*pril.*", "[DELETED]")
str_extract_all(item, "[0-9]+")
# read data (text + label)
idash <- read.table("idash.txt", sep = "\t", header = FALSE, comment.char="", stringsAsFactors = FALSE)
# put texts in the variable "data" -> this will be the material for building model
data <- idash$V1
# put labels in the variable "label" -> this will be the ground truth of machine learning
label <- as.factor(idash$V2)
# read data (text + label)
idash <- read.table("idash.txt", sep = "\t", header = FALSE, comment.char="", stringsAsFactors = FALSE)
# put texts in the variable "data" -> this will be the material for building model
data <- idash$V1
# put labels in the variable "label" -> this will be the ground truth of machine learning
label <- as.factor(idash$V2)
library(tm)
library(SnowballC)
# transform into the format that tm can process
corpus <- Corpus(VectorSource(data))
corpus <- tm_map(corpus, tolower)
# remove all metadata that we don't need
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
# remove stopwords
# you can also add other words that you want to remove, for example, "apple"
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), "apple"))
# remember to import "SnowballC" library before stemming
corpus <- tm_map(corpus, stemDocument)
corpus
# generate the default document-term matrix with bag-of-words and term frequency
dtm <- DocumentTermMatrix(corpus)
dtm
findFreqTerms(dtm, lowfreq=300)
# generate the document-term matrix with bag-of-words and tf-idf weighting
dtm_tfidf <- DocumentTermMatrix(corpus,
control=list(weighting=function(x) weightTfIdf(x, normalize=TRUE),
stopwords=TRUE))
# show the most "important" words
findFreqTerms(dtm_tfidf, lowfreq=2.5)
# generate the document-term matrix with bigram and term frequency
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm_bigram <- DocumentTermMatrix(corpus, control = list(tokenize = BigramTokenizer))
findFreqTerms(dtm_bigram, lowfreq=100)
# generate the document-term matrix with bag-of-words + bigram and term frequency
UniBigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
dtm_unibigram <- DocumentTermMatrix(corpus, control = list(tokenize = UniBigramTokenizer))
findFreqTerms(dtm_unibigram, lowfreq=300)
dtm <- removeSparseTerms(dtm, 0.995)
dtm
findAssocs(dtm, "ventricular", corlimit=0.5)
library(wordcloud)
df <- suppressWarnings(data.frame(as.matrix(dtm)))
wordcloud(colnames(df), colSums(df), scale=c(5, 1), max.words=50, min.freq=10,
color=brewer.pal(6, "Dark2"), vfont=c("sans serif", "plain"))
library(wordcloud)
df <- suppressWarnings(data.frame(as.matrix(dtm)))
wordcloud(colnames(df), colSums(df), scale=c(5, 1), max.words=50, min.freq=10,
color=brewer.pal(6, "Dark2"), vfont=c("sans serif", "plain"))
library(ggplot2)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq > 300), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
freq
wf <- data.frame(word=names(freq), freq=freq)
wf
p <- ggplot(subset(wf, freq > 300), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
library(cluster)
library(fpc)
d <- dist(t(dtm), method="euclidian")
km <- kmeans(d, 6)
clusplot(as.matrix(d), km$cluster, color=T, shade=T, labels=2, lines=0)
knitr::opts_chunk$set(echo = TRUE)
# need to set your own path
setwd("~/git/2017_cebu/")
# the package we will use in the workshop
.packages <- c("tm", "SnowballC", "topicmodels", "tidytext", "caret", "rpart",
"text2vec", "Rtsne")
# install the package if it is not in the local R
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)
if ("keras" %in% rownames(installed.packages()) == FALSE) {
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
}
# file paths of sample data
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
download.file(url=fpath, destfile='id.txt', method='curl')
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/idash.txt"
download.file(url=fpath, destfile='idash.txt', method='curl')
1:nrow(idash)
idash <- read.table("idash.txt", sep = "\t", header = FALSE, comment.char="", stringsAsFactors = FALSE)
# store texts to "data"
data <- idash$V1
# store labels to "label"
label <- as.factor(idash$V2)
# as the previous workshop, we use tm to build document-term matrix
library(tm)
library(SnowballC)
corpus <- Corpus(VectorSource(data))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, c(stopwords("english")))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
# add the document name (just simply use the number)
rownames(dtm) <- 1:nrow(idash)
rownames(dtm)
dtm
library(topicmodels)
# build LDA model
lda_model <- LDA(dtm, k = 6, control = list(seed = 777))
# show assigned clusters of the first 50 documents
lda_topics <- topics(lda_model, 1)
lda_topics[1:50]
lda_keywords <- data.frame(terms(lda_model, 30), stringsAsFactors = FALSE)
lda_keywords
library(tidytext)
beta <- tidy(lda_model, matrix = "beta")
beta[1:20, ]
gamma <- tidy(lda_model, matrix = "gamma")
gamma[c(1:5, (431+1):(431+5), (431*2+1):(431*2+5), (431*3+1):(431*3+5)),]
d <- data.frame(as.matrix(dtm))
d$label <- label
# build a decision tree
library(caret)
library(rpart)
# split the dataset based on the label (70% training, 30% testing)
set.seed(123)
inTraining <- createDataPartition(d$label, p=0.7, list=F)
training <- d[ inTraining, ]
testing <- d[-inTraining, ]
# build the decision tree
model <- rpart(label~., data=training)
#summary(model)
# plot the decision tree
plot(model, uniform=TRUE)
text(model, use.n=TRUE)
tst <- testing
tst$label = NULL
# evaluate the performance
pred <- predict(model, tst, type="class")
confusionMatrix(testing$label, pred)
x <- readChar("id.txt", file.info("id.txt")$size)
x <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\nSTART_OF_RECORD=[0-9]+\\|\\|\\|\\|[0-9]+\\|\\|\\|\\|\n", " [split] ", x)
x <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\n", "", x)
x <- strsplit(x, " \\[split\\] ")
x <- x[[1]]
library(tm)
library(SnowballC)
library(topicmodels)
corpus <- Corpus(VectorSource(x))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, c(stopwords("english")))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
lda_model <- LDA(dtm, k = 5, control = list(seed = 777))
lda_keywords <- data.frame(terms(lda_model, 10), stringsAsFactors = FALSE)
lda_keywords
x_df <- as.data.frame(x)
x_df$label <- topics(lda_model, 1)
library(text2vec)
tokens <- space_tokenizer(data)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5)
vectorizer <- vocab_vectorizer(vocab,
grow_dtm = FALSE,
skip_grams_window = 5)
tcm <- create_tcm(it, vectorizer)
word_embedding_size <- 200
glove = GlobalVectors$new(word_vectors_size = word_embedding_size, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
word_vectors <- glove$get_word_vectors()
unknown <- word_vectors["mitral", , drop = FALSE] -
word_vectors["left", , drop = FALSE] +
word_vectors["right", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = unknown, method = "cosine", norm = "l2")
head(sort(cos_sim[, 1], decreasing = TRUE), 5)
library(text2vec)
tokens <- space_tokenizer(x)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5)
vectorizer <- vocab_vectorizer(vocab,
grow_dtm = FALSE,
skip_grams_window = 5)
tcm <- create_tcm(it, vectorizer)
word_embedding_size <- 200
glove = GlobalVectors$new(word_vectors_size = word_embedding_size, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
word_vectors <- glove$get_word_vectors()
unknown <- word_vectors["heart", , drop = FALSE] -
word_vectors["left", , drop = FALSE] +
word_vectors["right", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = unknown, method = "cosine", norm = "l2")
head(sort(cos_sim[, 1], decreasing = TRUE), 10)
library(keras)
maxlen <- 200
encoding_dim <- 32
batch_size <- 10
epochs <- 20
tok <- text_tokenizer(2000, lower = TRUE, split = " ", char_level = FALSE)
fit_text_tokenizer(tok, data)
data_idx <- texts_to_sequences(tok, data)
data_idx <- data_idx %>%
pad_sequences(maxlen = maxlen)
head(data_idx)
model <- keras_model_sequential()
model %>%
layer_dense(name = 'e1', units = encoding_dim, activation = 'relu', input_shape = maxlen) %>%
layer_dense(name = 'd1', units = maxlen, activation = 'sigmoid')
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam"
)
summary(model)
```
history <- model %>% fit(
x_train, x_train,
batch_size = batch_size,
epochs = epochs,
shuffle = TRUE,
validation_data = list(x_test, x_test)
)
plot(history)
```
inTraining <- 1:floor(nrow(data_idx) * 0.7)
x_train <- data_idx[inTraining, ]
x_test <- data_idx[-inTraining, ]
```
history <- model %>% fit(
x_train, x_train,
batch_size = batch_size,
epochs = epochs,
shuffle = TRUE,
validation_data = list(x_test, x_test)
)
plot(history)
layer_name <- 'e1'
intermediate_layer_model <- keras_model(inputs = model$input,
outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
dim(intermediate_output)
head(intermediate_output)
colors <- rainbow(length(unique(as.factor(label))))
pca <- princomp(intermediate_output)$scores[, 1:2]
plot(pca, t='n', main="pca")
text(pca, labels=label, col=colors[label])
library(Rtsne)
tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30,
verbose = TRUE, max_iter = 500)
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
model <- keras_model_sequential()
model %>%
layer_dense(name = 'e1', units = 128, activation = 'relu', input_shape = maxlen) %>%
layer_dense(name = 'e2', units = 64, activation = 'relu') %>%
layer_dense(name = 'e3', units = encoding_dim, activation = 'relu') %>%
layer_dense(name = 'd1', units = 64, activation = 'relu') %>%
layer_dense(name = 'd2', units = 128, activation = 'relu') %>%
layer_dense(name = 'd3', units = maxlen, activation = 'sigmoid')
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adadelta"
)
summary(model)
history <- model %>% fit(
x_train, x_train,
batch_size = batch_size,
epochs = epochs,
shuffle = TRUE,
validation_data = list(x_test, x_test)
)
plot(history)
layer_name <- 'e3'
intermediate_layer_model <- keras_model(inputs = model$input,
outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30,
verbose = TRUE, max_iter = 500)
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
