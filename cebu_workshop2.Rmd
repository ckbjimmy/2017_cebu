---
title: "Workshop 2 - Machine Learning-Based Approaches for Text Mining"
author: "Wei-Hung Weng"
date: "July 5, 2017"
output: 
  html_document: 
    fig_height: 8
    fig_width: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# need to set your own path
setwd("~/git/2017_cebu/")

# the package we will use in the workshop
.packages <- c("tm", "SnowballC", "topicmodels", "tidytext", "caret", "rpart",
               "text2vec", "Rtsne")

# install the package if it is not in the local R
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) {
  install.packages(.packages[!.inst], repos = "http://cran.rstudio.com")
}
lapply(.packages, library, character.only=TRUE)

if ("keras" %in% rownames(installed.packages()) == FALSE) {
  devtools::install_github("rstudio/keras")
  library(keras)
  install_tensorflow()
}

# file paths of sample data
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/id.txt"
download.file(url=fpath, destfile='id.txt', method='curl')
fpath <- "https://raw.githubusercontent.com/ckbjimmy/2017_cebu/master/data/idash.txt"
download.file(url=fpath, destfile='idash.txt', method='curl')
```


## Objectives

Participants may leave this workshop with skills to:

- Perform the **topic modeling** algorithm on the textual data in R  
- Understand the concept of **word embedding** representation and apply it in R  
- Learn a **hidden representation of deep neural network** from textual data in R  


## Instructions

**Before beginning**, please test to see if the Rmd file will compile on your system by clicking the "Knit HTML button" in R studio above.  


## Machine learning approaches for text mining

Machine learning is a powerful tool for text mining, especially finding the hidden representation, which is impossible if we just use regular expressions or simple NLP techniques.  

In this workshop, we will use machine learning algorithms to perform topic modeling, word embedding and extracting a hidden representation of the text using deep autoencoder. We will use the iDASH dataset and R language for the workshop. First, we will import the iDASH data and labels (please see the previous workshop for the introduction of the dataset).  

```{r}
# read the iDASH data
idash <- read.table("idash.txt", sep = "\t", header = FALSE, comment.char="", stringsAsFactors = FALSE)
# store texts to "data"
data <- idash$V1
# store labels to "label"
label <- as.factor(idash$V2)
```


## Learning language model to find the theme of the document - topic modeling

Topic modeling is an unsupervised learning technique to summarize and organize a large amount of textual information. Topic modeling algorithms can help us identify the words/terms which can be grouped into the same cluster (refer to "topic") from a collection of documents, discover hidden topics among the documents, annotate/label the documents by the identified groups (topics), and therefore understand, summarize, separate, and organize bunch of textual data. In other words, topic modeling may help us find out a hidden thematic representation of the document.  

We choose the Latent Dirichlet allocation (LDA) algorithm, which is a common and popular mathematical topic modeling method developed by Prof. David Blei, for topic modeling. LDA can estimate both of the followings at the same time: (1) each word in the document collection is attributable to one of the document's topics, and (2) each document (a set of words) is viewed as a mixture of topics that are present in the document collection. For example:  

- Document 1: a cardiology admission note, 80% cardiology, 15% nephrology, 4% pulmonology, 1% neurology  
- Document 2: a kidney biopsy report, 50% nephrology, 30% rheumatology, 15% infectious disease, 5% neurology  
- Document 3: a chest CT report, 50% pulmonology, 40% cardiology, 5% oncology, 5% infectious disease  
- ...  

Here each clinical document has multiple topics with different proportion. In this example, medical specialties can be regarded as natural topics of the document collection. If we dive into keywords of each topic, you may see something like:  

- Cardiology: ventricle, ekg, atrial, catheterization, cabg, edema, ...  
- Nephrology: hemodialysis, renal, edema, nephritis, sonography, dyspnea, ...  
- Pulmonology: dyspnea, lung, bronchitis, adenocarcinoma, egfr, edema, ...  
- Infectious disease: hiv, hepatitis, viral, fever, leukocytosis, ...  
- ...  

To be noticed, words can be shared across topics.  

We use the `LDA` function in `topicmodels` package to generate an LDA topic model. The `LDA` function takes a document-term matrix as an input, which can be generated using `tm` package that we introduced at the end of the previous workshop.  

```{r}
# as the previous workshop, we use tm to build document-term matrix
library(tm)
library(SnowballC)
corpus <- Corpus(VectorSource(data))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, c(stopwords("english")))
corpus <- tm_map(corpus, stemDocument)

dtm <- DocumentTermMatrix(corpus)
# add the document name (just simply use the number)
rownames(dtm) <- 1:nrow(idash)
```

In the LDA algorithm, we need to assign the number of possible natural topics (`k`) to `LDA` function. For example, we set `k = 6` to create a six-topic LDA model. You may use `topics` function to get the topic of each document, and apply `terms` function to see keywords of each topic (here we show top 30 keywords).   

```{r}
library(topicmodels)
# build LDA model
lda_model <- LDA(dtm, k = 6, control = list(seed = 777))
# show assigned clusters of the first 50 documents
lda_topics <- topics(lda_model, 1)
lda_topics[1:50]
# get keywords of each LDA topic
lda_keywords <- data.frame(terms(lda_model, 30), stringsAsFactors = FALSE)
lda_keywords
```

As you can see in `lda_topics`, most of first 50 documents are assigned to topic 5 or 6. From `lda_keywords`, topic 3 seems like gastroenterology, topic 4 might be neurology, topic 1 is psychology, topic 5 and 6 are cardiology, topic 2 is unclear.  

(Optional) We may also take the advantage of `tidytext` package to identify how words are associated with topics and how topics are associated with documents. `tidy` function with the argument `matrix = "beta"` returns the per-topic-per-word probabilities (word-topic probabilities), called "beta", from the LDA model. The `tidy` function may also gives you the per-document-per-topic probabilities (document-topic probabilities, called "gamma") if you use the argument `matrix = "gamma"`. For example, in this sample LDA model we see that the term `aaaa` belongs to topic 2 (from "beta"), and document 2 belongs to topic 2 (from "gamma", probability > 0.9996).  

```{r}
library(tidytext)
beta <- tidy(lda_model, matrix = "beta")
beta[1:20, ]
gamma <- tidy(lda_model, matrix = "gamma")
gamma[c(1:5, (431+1):(431+5), (431*2+1):(431*2+5), (431*3+1):(431*3+5)),]
```

LDA-derived topics/clusters can be used as natural labels for machine classification (supervised learning). We run a simple decision tree as an example. The accuracy is 0.7874, which is not that bad.   
```{r}
d <- data.frame(as.matrix(dtm))
d$label <- label

# build a decision tree
library(caret)
library(rpart)
# split the dataset based on the label (70% training, 30% testing)
set.seed(123)
inTraining <- createDataPartition(d$label, p=0.7, list=F)
training <- d[ inTraining, ]
testing <- d[-inTraining, ]

# build the decision tree
model <- rpart(label~., data=training)
#summary(model)
# plot the decision tree
plot(model, uniform=TRUE)
text(model, use.n=TRUE)

tst <- testing
tst$label = NULL
# evaluate the performance
pred <- predict(model, tst, type="class")
confusionMatrix(testing$label, pred)
```

### Exercise

Again, we use the [**PhysioNet Deidentified Medical Text**](https://physionet.org/works/DeidentifiedMedicalText/) as an example for the exercise. 

```{r}
x <- readChar("id.txt", file.info("id.txt")$size)
x <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\nSTART_OF_RECORD=[0-9]+\\|\\|\\|\\|[0-9]+\\|\\|\\|\\|\n", " [split] ", x)
x <- gsub("\n\n\\|\\|\\|\\|END_OF_RECORD\n\n", "", x)
x <- strsplit(x, " \\[split\\] ")
x <- x[[1]]
```

Try to use LDA to find the topic words (assuming that there are 5 topics), and find the topic of each document.  

### Answer

```{r}
library(tm)
library(SnowballC)
library(topicmodels)

corpus <- Corpus(VectorSource(x))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, c(stopwords("english")))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)

lda_model <- LDA(dtm, k = 5, control = list(seed = 777))

lda_keywords <- data.frame(terms(lda_model, 10), stringsAsFactors = FALSE)
lda_keywords

# use the result of LDA for supervised learning
x_df <- as.data.frame(x)
x_df$label <- topics(lda_model, 1)
```


## Learning language model for word vector representation - word embedding  

Word embedding is a natural language modeling method for word vector representation. Simply to say, it is a technique to represent words as vectors. Instead of very sparse one-hot representation or frequency count representation (bag-of-words), word embedding approach may encode each word of the large corpus into a dense vector, with semantics and linguistic regularities that bag-of-words can't achieve. One of the most famous examples of word embedding might be the following operation:

$$ vector('paris') - vector('france') + vector('germany') = vector('berlin') $$

Please check [wikipedia introduction](https://en.wikipedia.org/wiki/Word_embedding) for more background knowledge of word embedding. The most popular word embedding algorithms, [word2vec](https://en.wikipedia.org/wiki/Word2vec) and [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/), were developed by Mikolov at Google and Pennington at Stanford, respectively.  

In Python, you can use `gensim` or `tensorflow` for word2vec implementation. Now, we'd like to introduce `text2vec` package, which is a R package that using C++ as the backend for GloVe implementation. `text2vec` will help you process raw data through the following steps:  

1. Create a vocabulary set that we want to learn word vectors  

    * the words should not be too uncommon: use the `term_count_min` argument in `prune_vocabulary` function   

```{r}
library(text2vec)
tokens <- space_tokenizer(data)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5)
```

2. Next, we construct a sparse term-cooccurence matrix (TCM)  

    * you may decide the size of skip grams window via `skip_grams_window` argument  
    * the concept of `skip_grams_window = 5` can be imagined as: the central word can be inferred from left 5 and right 5 words  

```{r}
vectorizer <- vocab_vectorizer(vocab, 
                               grow_dtm = FALSE, 
                               skip_grams_window = 5)
tcm <- create_tcm(it, vectorizer)
```

3. Factorize the TCM using parallel stochastic gradient descent algorithm in GloVe, then fit the GloVe model  

    * `word_vectors_size` is an important argument that it will decide the output dimension of your word vectors, here we assign size of 200  
    * it will use all CPU cores on your machine but you can still specify it via `RcppParallel::setThreadOptions(numThreads = CORE_NUMBER)`  

```{r}
word_embedding_size <- 200

glove = GlobalVectors$new(word_vectors_size = word_embedding_size, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
```

4. Extract the word vectors  

```{r}
word_vectors <- glove$get_word_vectors()
```

Once we get word vectors, we can try to do something similar to "paris-france+germany" operation through calculating the cosine similarity between word vectors  

```{r}
unknown <- word_vectors["mitral", , drop = FALSE] - 
  word_vectors["left", , drop = FALSE] + 
  word_vectors["right", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = unknown, method = "cosine", norm = "l2")

head(sort(cos_sim[, 1], decreasing = TRUE), 5)
```

### Exercise

Build the GloVe word vector model for **PhysioNet Deidentified Medical Text** (saved in variable `x`). Try to tune the `skip_grams_window`, `word_vectors_size` and `n_iter` to optimize your result!

### Answer

```{r}
library(text2vec)
tokens <- space_tokenizer(x)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5)
vectorizer <- vocab_vectorizer(vocab, 
                               grow_dtm = FALSE, 
                               skip_grams_window = 5)
tcm <- create_tcm(it, vectorizer)
word_embedding_size <- 200
glove = GlobalVectors$new(word_vectors_size = word_embedding_size, vocabulary = vocab, x_max = 10)
glove$fit(tcm, n_iter = 20)
word_vectors <- glove$get_word_vectors()
unknown <- word_vectors["heart", , drop = FALSE] - 
  word_vectors["left", , drop = FALSE] + 
  word_vectors["right", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = unknown, method = "cosine", norm = "l2")
head(sort(cos_sim[, 1], decreasing = TRUE), 10)
```


## Learning hidden representation of deep neural network

Hidden layers of the neural network can learn an interesting respresentation of the data. We can use the hidden layer representation for many things, for example, dimensionality reduction and feature learning (though you may not understand what's the meaning of those hidden features). In this section we'd like to stack multiple neural network layers (multilayer autoencoder) to learn the hidden representation of data.  

The concepts, theories and algorithms of deep learning are not the focuses of this workshop. You may visit [this Quora thread](https://www.quora.com/What-is-deep-learning) to get some thoughts about deep learning if you are not familiar with it. If you want to learn more, Stanford provides [a detailed deep learning tutorial](http://deeplearning.stanford.edu/tutorial/), and Goodfellow et al. also wrote a very good (but not so easy) [Deep Learning textbook](http://www.deeplearningbook.org/) for extensive deep learning study.  

Here we choose Keras to build deep learning model due to its simplicity. Keras is a very high-level API for neural network architecture implementation, which is originally implemented in Python (but now also in R, thanks to RStudio team). It uses Google TensorFlow (in Python) as the backend in [R interface to Keras](https://rstudio.github.io/keras/).  

p.s. please rerun the following code if your R Studio can't import Keras  

```
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
```

In this section, we want to build a simple three layer autoencoder (only one hidden layer, with one input layer and one output layer) for encoding hidden representation. Please visit [Stanford UFLDL Tutorial: Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/), or read the [Deep Learning Book Chapter 14: Autoencoders](http://www.deeplearningbook.org/contents/autoencoders.html) if you want to learn more theories about autoencoder.

![](img/ae.png)

**Figure: An autoencoder with one hidden layer ($L_2$), courtesy by [Stanford UFLDL Tutorial: Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)**

First, we define some parameters before building the deep neural network model.  

- The `maxlen` should be equal or less than your input dimension. For example, we set up `200` since that most of the nursing notes have less than 200 words.  
- The `encoding_dim` is the number of the output dimension. We choose `32` to get the 32-dimension vector for each document.  
- Therefore, all documents will be truncated to 200 words (at most), and the 200-dimension vectors will be encoded into 32-dimension vectors.  
- `batch_size` and `epochs` are training parameters, you may change them to get better results.  

```{r}
library(keras)

maxlen <- 200
encoding_dim <- 32
batch_size <- 10
epochs <- 20
```

Next, we need to prepare the input data for deep neural network. Free text needs to be converted to index sequence for Keras input. `text_tokenizer` and `fit_text_tokenizer` functions help us tokenize our raw data, and `texts_to_sequences` function converts our textual data to a list of index sequences. Since lengths of nursing notes are all different, we use `pad_sequences` to pad the sequences (pad the sequences with 0 to the left), and let all notes (now index sequences) have all the same length. You may use `head(data_idx)` to see how the data looks like after transformation. 

```{r}
tok <- text_tokenizer(2000, lower = TRUE, split = " ", char_level = FALSE)
fit_text_tokenizer(tok, data)
data_idx <- texts_to_sequences(tok, data)

data_idx <- data_idx %>%
  pad_sequences(maxlen = maxlen)

head(data_idx)
```

Then we split the data into training (70%) and validation sets (30%).  

```{r}
inTraining <- 1:floor(nrow(data_idx) * 0.7)
x_train <- data_idx[inTraining, ]
x_test <- data_idx[-inTraining, ]
```

Now we can build a neural network structure with one hidden layer! We use `keras_model_sequential` function to initialize a new Keras model. You need to do it every time before constructing your new model.  

In the R version of Keras, you can use `%>%` to stack the neural layer to your model. Imagine that your model is empty in the beginning, and you stack layer `e1`, then `d1` on it. Here the `e1` layer is the encoder, which inputs your list of index sequences, and output a hidden representation (with the dimension size you've assigned, here `32`). The `d1` layer is the decoder, which may take the hidden representation generated by the encoder, and try to reconstruct the original information (your input, with dimension size = `maxlen`).  

The algorithm tries to minimize the difference (technically we call it 'cost') between your original input and the result of reconstruction. We use `adam` as an optimizer, and evaluate the cost by `binary_crossentropy` for this important step. There are also many different optimizers, such as `sgd`, `rmsprop`, `adadelta`, ... that you can do experiments. Now the model is generated, and you can use `summary` function to see how it looks like.  

```{r}
# initialize the model
model <- keras_model_sequential()

model %>%
  layer_dense(name = 'e1', units = encoding_dim, activation = 'relu', input_shape = maxlen) %>%
  layer_dense(name = 'd1', units = maxlen, activation = 'sigmoid')

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam"
)

summary(model)
```

Then we use our data to fit the model, `x_train` as training data and `x_test` for validation. You may change `batch_size` and `epochs` to get better results. This model fitting step may take minutes to run.  
```{r}
history <- model %>% fit(
  x_train, x_train,
  batch_size = batch_size,
  epochs = epochs,
  shuffle = TRUE, 
  validation_data = list(x_test, x_test)
)

plot(history)
```

Once the autoencoder is constructed and optimized by the data, we can now extract the hidden representation from the model. The hidden representation is the output of the encoder, which is the output of `e1` layer. We can use `keras_model` and `predict` functions to get the representation and save it in the variable `intermediate_output`.  

```{r}
layer_name <- 'e1'

intermediate_layer_model <- keras_model(inputs = model$input,
                                        outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
dim(intermediate_output)
head(intermediate_output)
```

We can visualize how the documents located in the vector space using principal component analysis (PCA) (using `princomp` function, `scores[, 1:2]` for first two principal components).  

```{r}
colors <- rainbow(length(unique(as.factor(label))))

pca <- princomp(intermediate_output)$scores[, 1:2]
plot(pca, t='n', main="pca")
text(pca, labels=label, col=colors[label])
```

t-SNE is an awesome dimension reduction algorithm if you want to visualize how the hidden representation looks like in the vector space. The R implementation of t-SNE algorithm is in `Rtsne` package. Let's see how it performs compared to PCA.  

```{r}
library(Rtsne)

tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30, 
              verbose = TRUE, max_iter = 500)

plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
```

### Exercise

The autoencoder can be extended to not only single hidden layer. Let's try to implement an autoencoder with 5 hidden layers instead of just one (encode/decode 3 times, respectively). The output of each layer can be a unique feature representation. However, in general, we usually pick up the middle-most layer with the smallest size of dimension as the representation we want (here the output of `e3` layer).  

### Answer

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(name = 'e1', units = 128, activation = 'relu', input_shape = maxlen) %>%
  layer_dense(name = 'e2', units = 64, activation = 'relu') %>%
  layer_dense(name = 'e3', units = encoding_dim, activation = 'relu') %>%
  layer_dense(name = 'd1', units = 64, activation = 'relu') %>%
  layer_dense(name = 'd2', units = 128, activation = 'relu') %>%
  layer_dense(name = 'd3', units = maxlen, activation = 'sigmoid')

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adadelta"
)

summary(model)

history <- model %>% fit(
  x_train, x_train,
  batch_size = batch_size,
  epochs = epochs,
  shuffle = TRUE, 
  validation_data = list(x_test, x_test)
)
plot(history)

layer_name <- 'e3'
intermediate_layer_model <- keras_model(inputs = model$input,
                                        outputs = get_layer(model, layer_name)$output)
intermediate_output <- predict(intermediate_layer_model, x_train)

tsne <- Rtsne(as.matrix(intermediate_output), dims = 2, perplexity = 30, 
              verbose = TRUE, max_iter = 500)

plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=label, col=colors[label])
```


## Conclusion

Thank you for chekcing out the second workshop. We hope that you are much familiar with laguage modeling and deep learning using R.  
